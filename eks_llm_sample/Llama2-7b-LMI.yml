---
apiVersion: v1
kind: Pod
metadata:
  name: lmi-inference
  labels:
    app: lmi-inference
spec:
  tolerations:
    - key: "sagemaker.amazonaws.com/node-health-status"
      operator: "Equal"
      value: "unhealthy"
      effect: "NoSchedule"
  containers:
    - name: lmi-inference
      image: deepjavalibrary/djl-serving:0.28.0-lmi
      resources:
        limits:
          nvidia.com/gpu: 1
        requests:
          cpu: "2"
          memory: 10Gi
          nvidia.com/gpu: 1
          ephemeral-storage: 20Gi
      ports:
        - containerPort: 80
          name: http
      volumeMounts:
        - name: model
          mountPath: /opt/ml/model
        - name: shm
          mountPath: /dev/shm
      env:
        - name: OPTION_MODEL_ID
          value: "TheBloke/Llama-2-7B-Chat-fp16"
  volumes:
    - name: model
      hostPath:
       path: /opt/dlami/nvme
       type: DirectoryOrCreate
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 12Gi
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  restartPolicy: Never
---
apiVersion: v1
kind: Service
metadata:
  name: lmi-inference
spec:
  ports:
    - port: 8080
      protocol: TCP
      targetPort: http
  selector:
    app: text-inference
  type: ClusterIP
